{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vearUInGGLA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.applications import EfficientNetB3, ResNet152V2, Xception\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix, classification_report,\n",
        "    accuracy_score, precision_recall_fscore_support,\n",
        "    roc_auc_score, cohen_kappa_score\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "\n",
        "IMG_SIZE = (224, 224)\n",
        "INPUT_SHAPE = (224, 224, 3)\n",
        "NUM_CLASSES = 4\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 30\n",
        "LEARNING_RATE = 0.001\n",
        "DROPOUT_RATE = 0.45\n",
        "SENET_REDUCTION_RATIO = 16\n",
        "\n",
        "# Data directory\n",
        "DATA_DIR = \"-\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"WEATHER CLASSIFICATION - EXACT PAPER IMPLEMENTATION\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Image Size: {IMG_SIZE}\")\n",
        "print(f\"Batch Size: {BATCH_SIZE}\")\n",
        "print(f\"Epochs: {EPOCHS}\")\n",
        "print(f\"Learning Rate: {LEARNING_RATE}\")\n",
        "print(f\"Dropout Rate: {DROPOUT_RATE}\")\n",
        "print(f\"SENet Reduction Ratio: {SENET_REDUCTION_RATIO}\")\n",
        "print(\"=\" * 80)\n"
      ],
      "metadata": {
        "id": "WDLMn_-eGMbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 1. DATA LOADING AND PREPROCESSING\n",
        "# ============================================================================\n",
        "\n",
        "def load_dataset(data_dir):\n",
        "\n",
        "    X, y = [], []\n",
        "    class_names = sorted(os.listdir(data_dir))\n",
        "\n",
        "    print(f\"\\nLoading dataset from: {data_dir}\")\n",
        "    print(f\"Classes: {class_names}\")\n",
        "\n",
        "    for label, class_name in enumerate(class_names):\n",
        "        class_path = os.path.join(data_dir, class_name)\n",
        "        if not os.path.isdir(class_path):\n",
        "            continue\n",
        "\n",
        "        image_files = [f for f in os.listdir(class_path)\n",
        "                      if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "        for img_file in image_files:\n",
        "            img_path = os.path.join(class_path, img_file)\n",
        "\n",
        "\n",
        "            img = cv2.imread(img_path)\n",
        "            if img is None:\n",
        "                continue\n",
        "\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            img = cv2.resize(img, IMG_SIZE)\n",
        "            img = img.astype(np.float32) / 255.0\n",
        "\n",
        "            X.append(img)\n",
        "            y.append(label)\n",
        "\n",
        "        print(f\"  {class_name}: {len(image_files)} images\")\n",
        "\n",
        "    X = np.array(X, dtype=np.float32)\n",
        "    y = np.array(y, dtype=np.int32)\n",
        "\n",
        "    print(f\"\\nTotal images loaded: {len(X)}\")\n",
        "    print(f\"Image shape: {X[0].shape}\")\n",
        "\n",
        "    return X, y, class_names\n",
        "\n",
        "\n",
        "def create_data_generators():\n",
        "\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        rotation_range=20,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        vertical_flip=True,\n",
        "        zoom_range=0.2,\n",
        "        brightness_range=[0.8, 1.2],\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "\n",
        "\n",
        "    val_test_datagen = ImageDataGenerator()\n",
        "\n",
        "    return train_datagen, val_test_datagen\n"
      ],
      "metadata": {
        "id": "fcIdImYpGPln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 2. MODEL COMPONENTS\n",
        "# ============================================================================\n",
        "\n",
        "class SENetBlock(layers.Layer):\n",
        "\n",
        "\n",
        "    def __init__(self, reduction_ratio=16, **kwargs):\n",
        "        super(SENetBlock, self).__init__(**kwargs)\n",
        "        self.reduction_ratio = reduction_ratio\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        channels = input_shape[-1]\n",
        "\n",
        "\n",
        "        self.gap = layers.GlobalAveragePooling2D()\n",
        "\n",
        "\n",
        "        self.fc1 = layers.Dense(\n",
        "            channels // self.reduction_ratio,\n",
        "            activation='relu',\n",
        "            use_bias=False,\n",
        "            name='se_fc1'\n",
        "        )\n",
        "        self.fc2 = layers.Dense(\n",
        "            channels,\n",
        "            activation='sigmoid',\n",
        "            use_bias=False,\n",
        "            name='se_fc2'\n",
        "        )\n",
        "\n",
        "        super(SENetBlock, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "\n",
        "        z = self.gap(inputs)  # (B, C)\n",
        "\n",
        "\n",
        "        s = self.fc1(z)       # (B, C/r)\n",
        "        s = self.fc2(s)       # (B, C)\n",
        "\n",
        "\n",
        "        s = tf.reshape(s, [-1, 1, 1, tf.shape(inputs)[-1]])\n",
        "\n",
        "\n",
        "        output = inputs * s\n",
        "\n",
        "        return output\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(SENetBlock, self).get_config()\n",
        "        config.update({'reduction_ratio': self.reduction_ratio})\n",
        "        return config\n",
        "\n",
        "\n",
        "class AttentionBlock(layers.Layer):\n",
        "\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AttentionBlock, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        channels = input_shape[-1]\n",
        "\n",
        "\n",
        "        self.query_conv = layers.Conv2D(\n",
        "            channels, 1,\n",
        "            padding='same',\n",
        "            name='attention_query'\n",
        "        )\n",
        "        self.key_conv = layers.Conv2D(\n",
        "            channels, 1,\n",
        "            padding='same',\n",
        "            name='attention_key'\n",
        "        )\n",
        "        self.value_conv = layers.Conv2D(\n",
        "            channels, 1,\n",
        "            padding='same',\n",
        "            name='attention_value'\n",
        "        )\n",
        "\n",
        "        super(AttentionBlock, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        height = tf.shape(inputs)[1]\n",
        "        width = tf.shape(inputs)[2]\n",
        "        channels = tf.shape(inputs)[3]\n",
        "\n",
        "\n",
        "        query = self.query_conv(inputs)  # (B, H, W, C)\n",
        "        key = self.key_conv(inputs)      # (B, H, W, C)\n",
        "        value = self.value_conv(inputs)  # (B, H, W, C)\n",
        "\n",
        "\n",
        "        query = tf.reshape(query, [batch_size, -1, channels])  # (B, N, C)\n",
        "        key = tf.reshape(key, [batch_size, -1, channels])      # (B, N, C)\n",
        "        value = tf.reshape(value, [batch_size, -1, channels])  # (B, N, C)\n",
        "\n",
        "\n",
        "\n",
        "        attention_scores = tf.matmul(query, key, transpose_b=True)  # (B, N, N)\n",
        "\n",
        "\n",
        "        dk = tf.cast(channels, tf.float32)\n",
        "        attention_scores = attention_scores / tf.sqrt(dk)\n",
        "\n",
        "\n",
        "        attention_weights = tf.nn.softmax(attention_scores, axis=-1)\n",
        "\n",
        "\n",
        "        attention_output = tf.matmul(attention_weights, value)  # (B, N, C)\n",
        "\n",
        "\n",
        "        attention_output = tf.reshape(\n",
        "            attention_output,\n",
        "            [batch_size, height, width, channels]\n",
        "        )\n",
        "\n",
        "\n",
        "        output = inputs + attention_output\n",
        "\n",
        "        return output\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(AttentionBlock, self).get_config()\n",
        "        return config"
      ],
      "metadata": {
        "id": "LleDflt-GUw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 3. MODEL ARCHITECTURES\n",
        "# ============================================================================\n",
        "\n",
        "def create_baseline_efficientnetb3():\n",
        "\n",
        "    base_model = EfficientNetB3(\n",
        "        include_top=False,\n",
        "        weights='imagenet',\n",
        "        input_shape=INPUT_SHAPE,\n",
        "        pooling='avg'\n",
        "    )\n",
        "\n",
        "\n",
        "    base_model.trainable = False\n",
        "\n",
        "\n",
        "    inputs = layers.Input(shape=INPUT_SHAPE)\n",
        "    x = base_model(inputs, training=False)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dense(256, activation='relu')(x)\n",
        "    x = layers.Dropout(DROPOUT_RATE)(x)\n",
        "    outputs = layers.Dense(NUM_CLASSES, activation='softmax')(x)\n",
        "\n",
        "    model = models.Model(inputs, outputs, name='EfficientNetB3_FineTuned')\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def create_baseline_resnet152v2():\n",
        "\n",
        "    base_model = ResNet152V2(\n",
        "        include_top=False,\n",
        "        weights='imagenet',\n",
        "        input_shape=INPUT_SHAPE,\n",
        "        pooling='avg'\n",
        "    )\n",
        "\n",
        "\n",
        "    base_model.trainable = False\n",
        "\n",
        "\n",
        "    inputs = layers.Input(shape=INPUT_SHAPE)\n",
        "    x = base_model(inputs, training=False)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dense(256, activation='relu')(x)\n",
        "    x = layers.Dropout(DROPOUT_RATE)(x)\n",
        "    outputs = layers.Dense(NUM_CLASSES, activation='softmax')(x)\n",
        "\n",
        "    model = models.Model(inputs, outputs, name='ResNet152V2_FineTuned')\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def create_baseline_xception():\n",
        "\n",
        "    base_model = Xception(\n",
        "        include_top=False,\n",
        "        weights='imagenet',\n",
        "        input_shape=INPUT_SHAPE,\n",
        "        pooling='avg'\n",
        "    )\n",
        "\n",
        "\n",
        "    base_model.trainable = False\n",
        "\n",
        "\n",
        "    inputs = layers.Input(shape=INPUT_SHAPE)\n",
        "    x = base_model(inputs, training=False)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dense(256, activation='relu')(x)\n",
        "    x = layers.Dropout(DROPOUT_RATE)(x)\n",
        "    outputs = layers.Dense(NUM_CLASSES, activation='softmax')(x)\n",
        "\n",
        "    model = models.Model(inputs, outputs, name='Xception_FineTuned')\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def create_proposed_model():\n",
        "\n",
        "\n",
        "    base_model = Xception(\n",
        "        include_top=False,\n",
        "        weights='imagenet',\n",
        "        input_shape=INPUT_SHAPE\n",
        "    )\n",
        "\n",
        "\n",
        "    base_model.trainable = False\n",
        "\n",
        "\n",
        "    inputs = layers.Input(shape=INPUT_SHAPE, name='input')\n",
        "\n",
        "\n",
        "    x = base_model(inputs, training=False)\n",
        "\n",
        "\n",
        "    x = layers.BatchNormalization(name='bn_after_xception')(x)\n",
        "\n",
        "\n",
        "    x = SENetBlock(reduction_ratio=SENET_REDUCTION_RATIO, name='senet_block')(x)\n",
        "\n",
        "\n",
        "    x = AttentionBlock(name='attention_block')(x)\n",
        "\n",
        "\n",
        "    x = layers.GlobalAveragePooling2D(name='global_avg_pool')(x)\n",
        "\n",
        "\n",
        "    x = layers.Dense(256, activation='relu', name='dense_256')(x)\n",
        "\n",
        "\n",
        "    x = layers.Dropout(DROPOUT_RATE, name='dropout')(x)\n",
        "\n",
        "\n",
        "    outputs = layers.Dense(NUM_CLASSES, activation='softmax', name='output')(x)\n",
        "\n",
        "\n",
        "    model = models.Model(\n",
        "        inputs,\n",
        "        outputs,\n",
        "        name='Xception_SENet_Attention'\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "hG-EAIw6Gpcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 4. TRAINING FUNCTION\n",
        "# ============================================================================\n",
        "\n",
        "def train_model(model, X_train, y_train, X_val, y_val, model_name, class_weights=None):\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Training {model_name}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy',\n",
        "                keras.metrics.Precision(name='precision'),\n",
        "                keras.metrics.Recall(name='recall')]\n",
        "    )\n",
        "\n",
        "\n",
        "    callbacks = [\n",
        "        ModelCheckpoint(\n",
        "            f'{model_name}_best.h5',\n",
        "            monitor='val_accuracy',\n",
        "            save_best_only=True,\n",
        "            mode='max',\n",
        "            verbose=1\n",
        "        ),\n",
        "        ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.5,\n",
        "            patience=3,\n",
        "            min_lr=1e-7,\n",
        "            verbose=1\n",
        "        ),\n",
        "        CSVLogger(f'{model_name}_training_log.csv')\n",
        "    ]\n",
        "\n",
        "\n",
        "    train_datagen, val_datagen = create_data_generators()\n",
        "\n",
        "\n",
        "    train_generator = train_datagen.flow(\n",
        "        X_train, y_train,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    val_generator = val_datagen.flow(\n",
        "        X_val, y_val,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "\n",
        "    history = model.fit(\n",
        "        train_generator,\n",
        "        validation_data=val_generator,\n",
        "        epochs=EPOCHS,\n",
        "        callbacks=callbacks,\n",
        "        class_weight=class_weights,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "kQAz5LBgGxFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 5. EVALUATION FUNCTION\n",
        "# ============================================================================\n",
        "\n",
        "def evaluate_model(model, X_test, y_test, class_names, model_name):\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Evaluating {model_name} on Test Set\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "\n",
        "    y_pred_probs = model.predict(X_test, verbose=0)\n",
        "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "    y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "\n",
        "    kappa = cohen_kappa_score(y_true, y_pred)\n",
        "\n",
        "\n",
        "    precision, recall, f1, support = precision_recall_fscore_support(\n",
        "        y_true, y_pred, average=None\n",
        "    )\n",
        "\n",
        "\n",
        "    avg_precision = np.mean(precision)\n",
        "    avg_recall = np.mean(recall)\n",
        "    avg_f1 = np.mean(f1)\n",
        "\n",
        "\n",
        "    print(f\"\\nOverall Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "    print(f\"Cohen's Kappa:    {kappa:.4f}\")\n",
        "    print(f\"\\nAverage Metrics:\")\n",
        "    print(f\"  Precision: {avg_precision:.4f}\")\n",
        "    print(f\"  Recall:    {avg_recall:.4f}\")\n",
        "    print(f\"  F1-Score:  {avg_f1:.4f}\")\n",
        "\n",
        "    print(f\"\\nPer-Class Performance:\")\n",
        "    print(f\"{'Class':<12} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'Support':<12}\")\n",
        "    print(\"-\" * 60)\n",
        "    for i, class_name in enumerate(class_names):\n",
        "        print(f\"{class_name:<12} {precision[i]:<12.4f} {recall[i]:<12.4f} \"\n",
        "              f\"{f1[i]:<12.4f} {support[i]:<12.0f}\")\n",
        "\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    plt.figure(figsize=(8, 7))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_names,\n",
        "                yticklabels=class_names,\n",
        "                cbar_kws={'label': 'Count'})\n",
        "    plt.title(f'Confusion Matrix - {model_name}',\n",
        "              fontsize=14, fontweight='bold', pad=20)\n",
        "    plt.xlabel('Predicted Label', fontsize=12)\n",
        "    plt.ylabel('True Label', fontsize=12)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{model_name}_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    print(f\"\\nDetailed Classification Report:\")\n",
        "    print(classification_report(y_true, y_pred, target_names=class_names, digits=4))\n",
        "\n",
        "\n",
        "    try:\n",
        "        auc_roc = roc_auc_score(y_test, y_pred_probs, multi_class='ovr', average='macro')\n",
        "        print(f\"\\nAUC-ROC (macro): {auc_roc:.4f}\")\n",
        "    except:\n",
        "        print(\"\\nAUC-ROC: Could not compute\")\n",
        "        auc_roc = None\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'kappa': kappa,\n",
        "        'precision': avg_precision,\n",
        "        'recall': avg_recall,\n",
        "        'f1_score': avg_f1,\n",
        "        'confusion_matrix': cm,\n",
        "        'y_pred': y_pred,\n",
        "        'y_pred_probs': y_pred_probs,\n",
        "        'y_true': y_true,\n",
        "        'auc_roc': auc_roc\n",
        "    }"
      ],
      "metadata": {
        "id": "eHFlBSriG1mi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 5A. CALIBRATION METRICS - ECE (Expected Calibration Error)\n",
        "# ============================================================================\n",
        "\n",
        "def compute_ece(y_true, y_pred_probs, n_bins=10):\n",
        "\n",
        "\n",
        "    confidences = np.max(y_pred_probs, axis=1)\n",
        "    predictions = np.argmax(y_pred_probs, axis=1)\n",
        "    accuracies = predictions == y_true\n",
        "\n",
        "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
        "    bin_lowers = bin_boundaries[:-1]\n",
        "    bin_uppers = bin_boundaries[1:]\n",
        "\n",
        "    ece = 0.0\n",
        "    bin_data = {\n",
        "        'accuracies': [],\n",
        "        'confidences': [],\n",
        "        'counts': [],\n",
        "        'gaps': []\n",
        "    }\n",
        "\n",
        "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
        "\n",
        "        in_bin = (confidences > bin_lower) & (confidences <= bin_upper)\n",
        "        bin_count = np.sum(in_bin)\n",
        "\n",
        "        if bin_count > 0:\n",
        "\n",
        "            bin_accuracy = np.mean(accuracies[in_bin])\n",
        "\n",
        "            bin_confidence = np.mean(confidences[in_bin])\n",
        "\n",
        "            gap = np.abs(bin_confidence - bin_accuracy)\n",
        "\n",
        "\n",
        "            ece += (bin_count / len(y_true)) * gap\n",
        "\n",
        "            bin_data['accuracies'].append(bin_accuracy)\n",
        "            bin_data['confidences'].append(bin_confidence)\n",
        "            bin_data['counts'].append(bin_count)\n",
        "            bin_data['gaps'].append(gap)\n",
        "        else:\n",
        "            bin_data['accuracies'].append(0)\n",
        "            bin_data['confidences'].append(0)\n",
        "            bin_data['counts'].append(0)\n",
        "            bin_data['gaps'].append(0)\n",
        "\n",
        "    return ece, bin_data\n",
        "\n",
        "\n",
        "def plot_calibration_curve(y_true, y_pred_probs, model_name, n_bins=10, save_path=None):\n",
        "\n",
        "    ece, bin_data = compute_ece(y_true, y_pred_probs, n_bins)\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "\n",
        "    bin_centers = np.linspace(0, 1, n_bins + 1)[:-1] + 0.5 / n_bins\n",
        "\n",
        "    ax1.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration', linewidth=2)\n",
        "    ax1.bar(bin_centers, bin_data['accuracies'], width=1.0/n_bins,\n",
        "            alpha=0.7, edgecolor='black', label='Model Output')\n",
        "    ax1.set_xlabel('Confidence', fontsize=12, fontweight='bold')\n",
        "    ax1.set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
        "    ax1.set_title(f'Calibration Curve\\nECE = {ece:.4f}',\n",
        "                  fontsize=13, fontweight='bold')\n",
        "    ax1.legend(loc='upper left')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    ax1.set_xlim([0, 1])\n",
        "    ax1.set_ylim([0, 1])\n",
        "\n",
        "\n",
        "    confidences = np.max(y_pred_probs, axis=1)\n",
        "    ax2.hist(confidences, bins=20, edgecolor='black', alpha=0.7, color='steelblue')\n",
        "    ax2.set_xlabel('Confidence', fontsize=12, fontweight='bold')\n",
        "    ax2.set_ylabel('Count', fontsize=12, fontweight='bold')\n",
        "    ax2.set_title(f'Confidence Distribution\\nMean: {np.mean(confidences):.3f}',\n",
        "                  fontsize=13, fontweight='bold')\n",
        "    ax2.grid(True, alpha=0.3, axis='y')\n",
        "    ax2.set_xlim([0, 1])\n",
        "\n",
        "    plt.suptitle(f'Model Calibration - {model_name}',\n",
        "                 fontsize=14, fontweight='bold', y=1.02)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"\\nCalibration Metrics:\")\n",
        "    print(f\"  ECE (Expected Calibration Error): {ece:.4f}\")\n",
        "    print(f\"  Mean Confidence: {np.mean(confidences):.4f}\")\n",
        "    print(f\"  Max Confidence: {np.max(confidences):.4f}\")\n",
        "    print(f\"  Min Confidence: {np.min(confidences):.4f}\")\n",
        "\n",
        "    return ece\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 5B. SOFTMAX CONFIDENCE PLOT\n",
        "# ============================================================================\n",
        "\n",
        "def plot_softmax_confidence(y_true, y_pred_probs, class_names, model_name, save_path=None):\n",
        "\n",
        "    predictions = np.argmax(y_pred_probs, axis=1)\n",
        "    confidences = np.max(y_pred_probs, axis=1)\n",
        "\n",
        "\n",
        "    correct_mask = predictions == y_true\n",
        "    correct_conf = confidences[correct_mask]\n",
        "    incorrect_conf = confidences[~correct_mask]\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "\n",
        "    ax1 = axes[0, 0]\n",
        "    box_data = [correct_conf, incorrect_conf]\n",
        "    bp = ax1.boxplot(box_data, labels=['Correct', 'Incorrect'],\n",
        "                     patch_artist=True, showmeans=True)\n",
        "    bp['boxes'][0].set_facecolor('lightgreen')\n",
        "    bp['boxes'][1].set_facecolor('lightcoral')\n",
        "    ax1.set_ylabel('Confidence Score', fontsize=11, fontweight='bold')\n",
        "    ax1.set_title('Confidence Distribution: Correct vs Incorrect',\n",
        "                  fontsize=12, fontweight='bold')\n",
        "    ax1.grid(True, alpha=0.3, axis='y')\n",
        "    ax1.set_ylim([0, 1.05])\n",
        "\n",
        "\n",
        "    ax1.text(1, np.mean(correct_conf) + 0.05,\n",
        "             f'μ={np.mean(correct_conf):.3f}',\n",
        "             ha='center', fontweight='bold')\n",
        "    ax1.text(2, np.mean(incorrect_conf) + 0.05,\n",
        "             f'μ={np.mean(incorrect_conf):.3f}',\n",
        "             ha='center', fontweight='bold')\n",
        "\n",
        "\n",
        "    ax2 = axes[0, 1]\n",
        "    ax2.hist(correct_conf, bins=20, alpha=0.6, label='Correct',\n",
        "             color='green', edgecolor='black')\n",
        "    ax2.hist(incorrect_conf, bins=20, alpha=0.6, label='Incorrect',\n",
        "             color='red', edgecolor='black')\n",
        "    ax2.set_xlabel('Confidence Score', fontsize=11, fontweight='bold')\n",
        "    ax2.set_ylabel('Count', fontsize=11, fontweight='bold')\n",
        "    ax2.set_title('Confidence Histogram', fontsize=12, fontweight='bold')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "\n",
        "    ax3 = axes[1, 0]\n",
        "    class_confidences = []\n",
        "    for i in range(len(class_names)):\n",
        "        class_mask = y_true == i\n",
        "        class_conf = confidences[class_mask]\n",
        "        class_confidences.append(class_conf)\n",
        "\n",
        "    bp2 = ax3.boxplot(class_confidences, labels=class_names,\n",
        "                      patch_artist=True, showmeans=True)\n",
        "    colors = ['lightblue', 'lightgreen', 'lightyellow', 'lightcoral']\n",
        "    for patch, color in zip(bp2['boxes'], colors):\n",
        "        patch.set_facecolor(color)\n",
        "\n",
        "    ax3.set_ylabel('Confidence Score', fontsize=11, fontweight='bold')\n",
        "    ax3.set_xlabel('Class', fontsize=11, fontweight='bold')\n",
        "    ax3.set_title('Confidence by True Class', fontsize=12, fontweight='bold')\n",
        "    ax3.grid(True, alpha=0.3, axis='y')\n",
        "    ax3.set_ylim([0, 1.05])\n",
        "    plt.setp(ax3.xaxis.get_majorticklabels(), rotation=15, ha='right')\n",
        "\n",
        "\n",
        "    ax4 = axes[1, 1]\n",
        "    sorted_correct = np.sort(correct_conf)\n",
        "    sorted_incorrect = np.sort(incorrect_conf)\n",
        "\n",
        "    ax4.plot(sorted_correct, np.linspace(0, 1, len(sorted_correct)),\n",
        "             label='Correct', color='green', linewidth=2)\n",
        "    ax4.plot(sorted_incorrect, np.linspace(0, 1, len(sorted_incorrect)),\n",
        "             label='Incorrect', color='red', linewidth=2)\n",
        "    ax4.set_xlabel('Confidence Score', fontsize=11, fontweight='bold')\n",
        "    ax4.set_ylabel('Cumulative Probability', fontsize=11, fontweight='bold')\n",
        "    ax4.set_title('Cumulative Distribution Function', fontsize=12, fontweight='bold')\n",
        "    ax4.legend()\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "    ax4.set_xlim([0, 1])\n",
        "    ax4.set_ylim([0, 1])\n",
        "\n",
        "    plt.suptitle(f'Softmax Confidence Analysis - {model_name}',\n",
        "                 fontsize=14, fontweight='bold', y=1.00)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    print(f\"\\nSoftmax Confidence Statistics:\")\n",
        "    print(f\"  Correct Predictions:\")\n",
        "    print(f\"    Mean: {np.mean(correct_conf):.4f}\")\n",
        "    print(f\"    Std:  {np.std(correct_conf):.4f}\")\n",
        "    print(f\"    Min:  {np.min(correct_conf):.4f}\")\n",
        "    print(f\"  Incorrect Predictions:\")\n",
        "    print(f\"    Mean: {np.mean(incorrect_conf):.4f}\")\n",
        "    print(f\"    Std:  {np.std(incorrect_conf):.4f}\")\n",
        "    print(f\"    Max:  {np.max(incorrect_conf):.4f}\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 5C. ATTENTION HEATMAP VISUALIZATION\n",
        "# ============================================================================\n",
        "\n",
        "def extract_attention_heatmap(model, image):\n",
        "\n",
        "\n",
        "    attention_layer = None\n",
        "    for layer in model.layers:\n",
        "        if isinstance(layer, AttentionBlock):\n",
        "            attention_layer = layer\n",
        "            break\n",
        "\n",
        "    if attention_layer is None:\n",
        "        print(\"Warning: No AttentionBlock found in model\")\n",
        "        return None\n",
        "\n",
        "\n",
        "    attention_model = keras.Model(\n",
        "        inputs=model.input,\n",
        "        outputs=attention_layer.output\n",
        "    )\n",
        "\n",
        "\n",
        "    img_batch = np.expand_dims(image, axis=0)\n",
        "    attention_output = attention_model.predict(img_batch, verbose=0)\n",
        "\n",
        "\n",
        "    attention_map = np.mean(attention_output[0], axis=-1)  # (H, W)\n",
        "\n",
        "\n",
        "    attention_map = np.maximum(attention_map, 0)\n",
        "    attention_map = attention_map / (np.max(attention_map) + 1e-8)\n",
        "\n",
        "    return attention_map\n",
        "\n",
        "\n",
        "def plot_attention_overlay(model, X_samples, y_true, y_pred, class_names,\n",
        "                           model_name, num_samples=6, save_path=None):\n",
        "\n",
        "    num_samples = min(num_samples, len(X_samples))\n",
        "\n",
        "    fig, axes = plt.subplots(num_samples, 3, figsize=(12, 4*num_samples))\n",
        "\n",
        "    if num_samples == 1:\n",
        "        axes = axes.reshape(1, -1)\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        image = X_samples[i]\n",
        "        true_class = class_names[y_true[i]]\n",
        "        pred_class = class_names[y_pred[i]]\n",
        "\n",
        "\n",
        "        attention_map = extract_attention_heatmap(model, image)\n",
        "\n",
        "        if attention_map is None:\n",
        "            print(f\"Skipping sample {i+1}: Could not extract attention\")\n",
        "            continue\n",
        "\n",
        "\n",
        "        attention_resized = cv2.resize(\n",
        "            attention_map,\n",
        "            (image.shape[1], image.shape[0]),\n",
        "            interpolation=cv2.INTER_LINEAR\n",
        "        )\n",
        "\n",
        "\n",
        "        heatmap_uint8 = np.uint8(255 * attention_resized)\n",
        "        heatmap_colored_bgr = cv2.applyColorMap(heatmap_uint8, cv2.COLORMAP_JET)\n",
        "        heatmap_colored = cv2.cvtColor(heatmap_colored_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "\n",
        "        img_uint8 = np.uint8(image * 255)\n",
        "        overlay = cv2.addWeighted(img_uint8, 0.6, heatmap_colored, 0.4, 0)\n",
        "\n",
        "\n",
        "        axes[i, 0].imshow(img_uint8)\n",
        "        axes[i, 0].set_title(f'Original\\nTrue: {true_class}',\n",
        "                            fontsize=10, fontweight='bold')\n",
        "        axes[i, 0].axis('off')\n",
        "\n",
        "\n",
        "        im = axes[i, 1].imshow(attention_resized, cmap='jet', vmin=0, vmax=1)\n",
        "        axes[i, 1].set_title('Attention Heatmap', fontsize=10, fontweight='bold')\n",
        "        axes[i, 1].axis('off')\n",
        "\n",
        "\n",
        "        axes[i, 2].imshow(overlay)\n",
        "        color = 'green' if true_class == pred_class else 'red'\n",
        "        axes[i, 2].set_title(f'Overlay\\nPred: {pred_class}',\n",
        "                            fontsize=10, fontweight='bold', color=color)\n",
        "        axes[i, 2].axis('off')\n",
        "\n",
        "\n",
        "        if i == 0:\n",
        "            cbar = plt.colorbar(im, ax=axes[i, 1], fraction=0.046, pad=0.04)\n",
        "            cbar.set_label('Attention\\nIntensity', fontsize=8)\n",
        "            cbar.set_ticks([0, 0.5, 1])\n",
        "            cbar.set_ticklabels(['Low', 'Med', 'High'], fontsize=7)\n",
        "\n",
        "    plt.suptitle(f'Attention Heatmap Visualization - {model_name}',\n",
        "                 fontsize=14, fontweight='bold', y=0.995)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 5D. COMPREHENSIVE ANALYSIS FUNCTION\n",
        "# ============================================================================\n",
        "\n",
        "def comprehensive_analysis(model, X_test, y_test, class_names, model_name):\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"COMPREHENSIVE ANALYSIS: {model_name}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "\n",
        "    results = evaluate_model(model, X_test, y_test, class_names, model_name)\n",
        "\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"CALIBRATION ANALYSIS\")\n",
        "    print(f\"{'='*80}\")\n",
        "    ece = plot_calibration_curve(\n",
        "        results['y_true'],\n",
        "        results['y_pred_probs'],\n",
        "        model_name,\n",
        "        n_bins=10,\n",
        "        save_path=f'{model_name}_calibration.png'\n",
        "    )\n",
        "    results['ece'] = ece\n",
        "\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"SOFTMAX CONFIDENCE ANALYSIS\")\n",
        "    print(f\"{'='*80}\")\n",
        "    plot_softmax_confidence(\n",
        "        results['y_true'],\n",
        "        results['y_pred_probs'],\n",
        "        class_names,\n",
        "        model_name,\n",
        "        save_path=f'{model_name}_confidence.png'\n",
        "    )\n",
        "\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"ATTENTION HEATMAP VISUALIZATION\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "\n",
        "    correct_indices = np.where(results['y_pred'] == results['y_true'])[0]\n",
        "    incorrect_indices = np.where(results['y_pred'] != results['y_true'])[0]\n",
        "\n",
        "\n",
        "    selected_indices = []\n",
        "    if len(correct_indices) >= 4:\n",
        "        selected_indices.extend(np.random.choice(correct_indices, 4, replace=False))\n",
        "    else:\n",
        "        selected_indices.extend(correct_indices)\n",
        "\n",
        "    if len(incorrect_indices) >= 2:\n",
        "        selected_indices.extend(np.random.choice(incorrect_indices, 2, replace=False))\n",
        "    elif len(incorrect_indices) > 0:\n",
        "        selected_indices.extend(incorrect_indices)\n",
        "\n",
        "    if len(selected_indices) > 0:\n",
        "        selected_indices = selected_indices[:6]\n",
        "        plot_attention_overlay(\n",
        "            model,\n",
        "            X_test[selected_indices],\n",
        "            results['y_true'][selected_indices],\n",
        "            results['y_pred'][selected_indices],\n",
        "            class_names,\n",
        "            model_name,\n",
        "            num_samples=len(selected_indices),\n",
        "            save_path=f'{model_name}_attention_overlay.png'\n",
        "        )\n",
        "    else:\n",
        "        print(\"No samples available for attention visualization\")\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"ANALYSIS COMPLETE\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"\\nSummary for {model_name}:\")\n",
        "    print(f\"  Accuracy:      {results['accuracy']:.4f}\")\n",
        "    print(f\"  Cohen's Kappa: {results['kappa']:.4f}\")\n",
        "    print(f\"  Precision:     {results['precision']:.4f}\")\n",
        "    print(f\"  Recall:        {results['recall']:.4f}\")\n",
        "    print(f\"  F1-Score:      {results['f1_score']:.4f}\")\n",
        "    print(f\"  ECE:           {results['ece']:.4f}\")\n",
        "    if results['auc_roc']:\n",
        "        print(f\"  AUC-ROC:       {results['auc_roc']:.4f}\")\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "pcEPxbC6G7s7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 6. VISUALIZATION FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "def plot_training_history(history, model_name):\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "\n",
        "    axes[0, 0].plot(history.history['accuracy'], label='Train', color='blue')\n",
        "    axes[0, 0].plot(history.history['val_accuracy'], label='Validation', color='red')\n",
        "    axes[0, 0].set_title('Model Accuracy', fontweight='bold')\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Accuracy')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "\n",
        "    axes[0, 1].plot(history.history['loss'], label='Train', color='blue')\n",
        "    axes[0, 1].plot(history.history['val_loss'], label='Validation', color='red')\n",
        "    axes[0, 1].set_title('Model Loss', fontweight='bold')\n",
        "    axes[0, 1].set_xlabel('Epoch')\n",
        "    axes[0, 1].set_ylabel('Loss')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    axes[1, 0].plot(history.history['precision'], label='Train', color='blue')\n",
        "    axes[1, 0].plot(history.history['val_precision'], label='Validation', color='red')\n",
        "    axes[1, 0].set_title('Model Precision', fontweight='bold')\n",
        "    axes[1, 0].set_xlabel('Epoch')\n",
        "    axes[1, 0].set_ylabel('Precision')\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "\n",
        "    axes[1, 1].plot(history.history['recall'], label='Train', color='blue')\n",
        "    axes[1, 1].plot(history.history['val_recall'], label='Validation', color='red')\n",
        "    axes[1, 1].set_title('Model Recall', fontweight='bold')\n",
        "    axes[1, 1].set_xlabel('Epoch')\n",
        "    axes[1, 1].set_ylabel('Recall')\n",
        "    axes[1, 1].legend()\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.suptitle(f'Training History - {model_name}',\n",
        "                 fontsize=16, fontweight='bold', y=1.00)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{model_name}_training_history.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "MCQ0I6LDHIfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 7. MAIN EXECUTION\n",
        "# ============================================================================\n",
        "\n",
        "def main():\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"LOADING DATASET\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Load dataset"
      ],
      "metadata": {
        "id": "DQX_V1QhHOic"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}